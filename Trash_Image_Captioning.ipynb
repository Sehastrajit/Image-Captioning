{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lP_0nOJkLe6W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string,os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore',category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7TDMg_wMBXQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from numpy.random import seed\n",
        "tf.random.set_seed(2)\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "O4aBD__xMX7i"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "import tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "T1PADV1VN2Sw"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/lstm_text.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/lstm_text.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m b\u001b[38;5;241m=\u001b[39ma\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      3\u001b[0m l \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/lstm_text.txt'"
          ]
        }
      ],
      "source": [
        "a=open(\"/content/lstm_text.txt\",'r')\n",
        "b=a.read()\n",
        "l = b.split(\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PUn8LlMcS4S"
      },
      "outputs": [],
      "source": [
        "a=open(\"/content/Geng.txt\",'r')\n",
        "h=a.readlines()\n",
        "temp = [line[:-1] for line in h]\n",
        "temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUQKkrJ7FKEt"
      },
      "outputs": [],
      "source": [
        "print(l[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SscH3FeSPj0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moHTXNFVWIM8",
        "outputId": "2fa37135-6e99-4d87-cda2-9bc35e23c481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['here is an expanded text about cheeseburgers and pickles to around 5000 charactersthere is nothing quite like biting into a delicious cheeseburger loaded up with all the fixings especially when those fixings include crisp tangy pickles', ' the classic combo of a juicy beef patty melted cheese fresh bun and those green discs of briny goodness is one of lifes simple pleasures that brings smiles to people of all ages', ' its the allamerican comfort food that has been satisfying cravings for decades and will likely continue doing so for generations to come', ' a truly great cheeseburger starts with the beef patty itself  thick flavorful and cooked to perfection so it stays juicy inside while developing an irresistible seared crust on the exterior', ' the cheese whether its a slice of classic american a thick smear of velvety cheddar or something more gourmet like aged swiss provides a rich creamy blanket to complement the savory beef', ' nestled into a pillowy yet sturdy bun lightly toasted to provide warmth and an addictive butter aroma the foundations are in place for burger bliss', 'but the component that really takes a great cheeseburger over the top is the addition of crisp crunchy pickle slices', ' their bright green color and distinct ridges catch your eye immediately creating anticipation for that signature tart and vinegary punch', ' with each bite those pickle slices release small bursts of briny sourness that cuts through the richness of the beef and cheese', ' its a highlow flavor contrast that simply works invigorating the taste buds with its boldness']\n"
          ]
        }
      ],
      "source": [
        "def clean_text(txt):\n",
        "\n",
        "    txt = \"\".join(t for t in txt if t not in string.punctuation).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return txt\n",
        "corpus = [clean_text(x) for x in l]\n",
        "print(corpus[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzFH6F0xW9zz",
        "outputId": "4d8be6a2-009d-46c9-8cc1-450cf1aa5a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[79, 9], [79, 9, 19], [79, 9, 19, 80], [79, 9, 19, 80, 81], [79, 9, 19, 80, 81, 82], [79, 9, 19, 80, 81, 82, 83], [79, 9, 19, 80, 81, 82, 83, 3], [79, 9, 19, 80, 81, 82, 83, 3, 13], [79, 9, 19, 80, 81, 82, 83, 3, 13, 5], [79, 9, 19, 80, 81, 82, 83, 3, 13, 5, 37]]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "def get_sequence_of_tokens(corpus):\n",
        "    ## tokenization\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    ## convert data to a token sequence\n",
        "    input_sequences = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, total_words\n",
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
        "print(inp_sequences[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpLnrvg8ZHDS"
      },
      "outputs": [],
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    label = ku.to_categorical(label, num_classes=total_words)\n",
        "    return predictors, label, max_sequence_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4ZnNNq4Pcu_"
      },
      "outputs": [],
      "source": [
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pflRp1ouZIzF"
      },
      "outputs": [],
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "    input_len = max_sequence_len - 1\n",
        "    model = Sequential()\n",
        "    # ----------Add Input Embedding Layer\n",
        "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "    # ----------Add Hidden Layer 1 - LSTM Layer\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dropout(0.1))\n",
        "    # ----------Add Output Layer\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgWIvL23PmuQ",
        "outputId": "3ca288da-106d-4139-de31-5e122983b8db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 35, 10)            3070      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               44400     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 307)               31007     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 78477 (306.55 KB)\n",
            "Trainable params: 78477 (306.55 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IOJD3PmPxE5",
        "outputId": "efb5731e-1c3d-482a-c8eb-692b39fbad90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "Epoch 2/100\n",
            "Epoch 3/100\n",
            "Epoch 4/100\n",
            "Epoch 5/100\n",
            "Epoch 6/100\n",
            "Epoch 7/100\n",
            "Epoch 8/100\n",
            "Epoch 9/100\n",
            "Epoch 10/100\n",
            "Epoch 11/100\n",
            "Epoch 12/100\n",
            "Epoch 13/100\n",
            "Epoch 14/100\n",
            "Epoch 15/100\n",
            "Epoch 16/100\n",
            "Epoch 17/100\n",
            "Epoch 18/100\n",
            "Epoch 19/100\n",
            "Epoch 20/100\n",
            "Epoch 21/100\n",
            "Epoch 22/100\n",
            "Epoch 23/100\n",
            "Epoch 24/100\n",
            "Epoch 25/100\n",
            "Epoch 26/100\n",
            "Epoch 27/100\n",
            "Epoch 28/100\n",
            "Epoch 29/100\n",
            "Epoch 30/100\n",
            "Epoch 31/100\n",
            "Epoch 32/100\n",
            "Epoch 33/100\n",
            "Epoch 34/100\n",
            "Epoch 35/100\n",
            "Epoch 36/100\n",
            "Epoch 37/100\n",
            "Epoch 38/100\n",
            "Epoch 39/100\n",
            "Epoch 40/100\n",
            "Epoch 41/100\n",
            "Epoch 42/100\n",
            "Epoch 43/100\n",
            "Epoch 44/100\n",
            "Epoch 45/100\n",
            "Epoch 46/100\n",
            "Epoch 47/100\n",
            "Epoch 48/100\n",
            "Epoch 49/100\n",
            "Epoch 50/100\n",
            "Epoch 51/100\n",
            "Epoch 52/100\n",
            "Epoch 53/100\n",
            "Epoch 54/100\n",
            "Epoch 55/100\n",
            "Epoch 56/100\n",
            "Epoch 57/100\n",
            "Epoch 58/100\n",
            "Epoch 59/100\n",
            "Epoch 60/100\n",
            "Epoch 61/100\n",
            "Epoch 62/100\n",
            "Epoch 63/100\n",
            "Epoch 64/100\n",
            "Epoch 65/100\n",
            "Epoch 66/100\n",
            "Epoch 67/100\n",
            "Epoch 68/100\n",
            "Epoch 69/100\n",
            "Epoch 70/100\n",
            "Epoch 71/100\n",
            "Epoch 72/100\n",
            "Epoch 73/100\n",
            "Epoch 74/100\n",
            "Epoch 75/100\n",
            "Epoch 76/100\n",
            "Epoch 77/100\n",
            "Epoch 78/100\n",
            "Epoch 79/100\n",
            "Epoch 80/100\n",
            "Epoch 81/100\n",
            "Epoch 82/100\n",
            "Epoch 83/100\n",
            "Epoch 84/100\n",
            "Epoch 85/100\n",
            "Epoch 86/100\n",
            "Epoch 87/100\n",
            "Epoch 88/100\n",
            "Epoch 89/100\n",
            "Epoch 90/100\n",
            "Epoch 91/100\n",
            "Epoch 92/100\n",
            "Epoch 93/100\n",
            "Epoch 94/100\n",
            "Epoch 95/100\n",
            "Epoch 96/100\n",
            "Epoch 97/100\n",
            "Epoch 98/100\n",
            "Epoch 99/100\n",
            "Epoch 100/100\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c92a4565b70>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(predictors, label, epochs=100, verbose=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hLjKlHiZKgi"
      },
      "outputs": [],
      "source": [
        "# def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "#     for _ in range(next_words):\n",
        "#         token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "#         token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "#         predicted = model.predict_classes(token_list, verbose=0)\n",
        "\n",
        "#         output_word = \"\"\n",
        "#         for word, index in tokenizer.word_index.items():\n",
        "#             if index == predicted:\n",
        "#                 output_word = word\n",
        "#                 break\n",
        "#         seed_text += \" \" + output_word\n",
        "#     return seed_text.title()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0mDrJO2RcsH"
      },
      "outputs": [],
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len, tokenizer):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        y_pred = model.predict(token_list)\n",
        "        predicted = np.argmax(y_pred, axis=1)[0]\n",
        "\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text.title()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1wF3byrZMZ4",
        "outputId": "274da484-a578-4f5d-96a8-23c72b9f7302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 455ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Cheeseburger Its The Allamerican\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Pangs And Cravings The Cheeseburger Is\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Essential Its The Next Time\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Tasty Bite Those\n"
          ]
        }
      ],
      "source": [
        "print (generate_text(\"cheeseburger\", 3, model, max_sequence_len,tokenizer))\n",
        "print (generate_text(\"pangs and cravings\", 3, model, max_sequence_len,tokenizer))\n",
        "print (generate_text(\"essential\", 4, model, max_sequence_len,tokenizer))\n",
        "print (generate_text(\"tasty\", 2, model, max_sequence_len,tokenizer))\n",
        "# print (generate_text(\"new york\", 3, model, max_sequence_len))\n",
        "# print (generate_text(\"science and technology\", 5, model, max_sequence_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCU01NuVSlbv",
        "outputId": "b7943941-78e3-4a4d-da71-ccafcfdf3e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Around Maybe Its\n"
          ]
        }
      ],
      "source": [
        "print (generate_text(\"around\", 2, model, max_sequence_len,tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQHDm0O9gASE"
      },
      "source": [
        "CNN part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tda9AMCPgBo1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUKim8c9gSII"
      },
      "outputs": [],
      "source": [
        "class cnn(keras.Model):\n",
        "  def __init__(self,n):\n",
        "    super(cnn,self).__init__()\n",
        "\n",
        "    layer_1 = layers.Conv2D"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
